{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31e7de0",
   "metadata": {},
   "source": [
    "### Metrics (RMSE, MAE, Coverage, Width, Winkler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02259f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] n=4375 rows | 2020-02-09 → 2022-10-09 | per-category\n",
      "\n",
      "=== Overall (TEST) ===\n",
      " Model   RMSE    MAE  Coverage_Prob  Avg_Interval_Width  Winkler_Score\n",
      "   MCR 33.990 21.090          0.385              37.965        113.576\n",
      "MCR+ML 20.461 12.508          0.038               0.053        124.869\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Set Path\n",
    "MCR_PATH   = \"/Users/jinnanut/Desktop/Code/MCR_summary_weekly.xlsx\"\n",
    "HYB_PATH   = \"/Users/jinnanut/Desktop/Code/MCR+ML_summary_weekly.xlsx\"\n",
    "RAW_PATH   = \"/Users/jinnanut/Library/Mobile Documents/com~apple~CloudDocs/NUT/Exeter University/Master/Term 3/BEMM466 Business Project/Dissertation/Data/WeeklyData.xlsx\"\n",
    "OUTPUT_DIR = Path(\"/Users/jinnanut/Desktop/Code/model_eval_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAKE_PLOTS = True\n",
    "ALPHA = 0.20          # P10–P90 is an 80% nominal interval\n",
    "W_ANCHOR = \"W-SUN\"    # use the same weekly anchor \n",
    "\n",
    "# Set function for transform date and set metrics\n",
    "def _prep(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df\n",
    "\n",
    "def metrics(y, yhat, low, up, alpha=0.20):\n",
    "    y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
    "    low = np.asarray(low, float); up = np.asarray(up, float)\n",
    "    err = yhat - y\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "    coverage = float(np.mean((y >= low) & (y <= up)))\n",
    "    width = float(np.mean(up - low))\n",
    "    penalty = (2/alpha)*((low-y)*(y<low) + (y-up)*(y>up))  # Winkler (lower=better)\n",
    "    winkler = float(np.mean((up - low) + penalty))\n",
    "    return rmse, mae, coverage, width, winkler\n",
    "\n",
    "# Load forecasts\n",
    "df_mcr = _prep(pd.read_excel(MCR_PATH))\n",
    "df_hyb = _prep(pd.read_excel(HYB_PATH))\n",
    "\n",
    "# Required columns in each forecast file: date, mean, p10, p90\n",
    "for name, df in [(\"MCR\", df_mcr), (\"MCR+ML\", df_hyb)]:\n",
    "    need = {\"date\", \"mean\", \"p10\", \"p90\"}\n",
    "    miss = need - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"{name} is missing columns: {sorted(miss)}\")\n",
    "\n",
    "# Load ACTUALS from raw data\n",
    "raw = _prep(pd.read_excel(RAW_PATH))   # columns: date, category, quantity\n",
    "if not {\"date\", \"quantity\"}.issubset(raw.columns):\n",
    "    raise ValueError(\"RAW file must have at least 'date' and 'quantity' columns (plus 'category' if multiseries).\")\n",
    "\n",
    "has_cat = \"category\" in raw.columns and (\"category\" in df_mcr.columns) and (\"category\" in df_hyb.columns)\n",
    "\n",
    "# resample to weekly with the same anchor and build actuals DataFrame\n",
    "raw = raw.set_index(\"date\").sort_index()\n",
    "if has_cat:\n",
    "    actual_weekly = (\n",
    "        raw.groupby(\"category\")[\"quantity\"]\n",
    "            .resample(W_ANCHOR, label=\"right\", closed=\"right\")\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "    )\n",
    "else:\n",
    "    actual_weekly = (\n",
    "        raw[[\"quantity\"]]\n",
    "            .resample(W_ANCHOR, label=\"right\", closed=\"right\").sum()\n",
    "            .reset_index()\n",
    "    )\n",
    "actual_weekly.rename(columns={\"quantity\": \"actual_demand\", \"date\":\"date\"}, inplace=True)\n",
    "\n",
    "# Standardize forecast columns and merge\n",
    "def _std(df, suffix):\n",
    "    cols = [\"date\", \"mean\", \"p10\", \"p90\"]\n",
    "    if has_cat: cols = [\"date\", \"category\", \"mean\", \"p10\", \"p90\"]\n",
    "    out = df[cols].copy()\n",
    "    out = out.rename(columns={\n",
    "        \"mean\": f\"forecast_demand_{suffix}\",\n",
    "        \"p10\":  f\"p10_{suffix}\",\n",
    "        \"p90\":  f\"p90_{suffix}\",\n",
    "    })\n",
    "    return out\n",
    "\n",
    "mcr_std = _std(df_mcr, \"mcr\")\n",
    "hyb_std = _std(df_hyb, \"hyb\")\n",
    "\n",
    "keys = [\"date\"] + ([\"category\"] if has_cat else [])\n",
    "\n",
    "merged = (\n",
    "    mcr_std.merge(hyb_std, on=keys, how=\"inner\")\n",
    "           .merge(actual_weekly, on=keys, how=\"inner\")   # <-- TRUE actuals joined here\n",
    "           .sort_values(keys)\n",
    "           .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Drop any rows with missing interval bounds (rare after inner merges)\n",
    "merged = merged.dropna(subset=[\n",
    "    \"forecast_demand_mcr\",\"forecast_demand_hyb\",\"p10_mcr\",\"p90_mcr\",\"p10_hyb\",\"p90_hyb\",\"actual_demand\"\n",
    "]).copy()\n",
    "\n",
    "print(f\"[Eval] n={len(merged)} rows | {merged['date'].min().date()} → {merged['date'].max().date()} | \"\n",
    "      f\"{'per-category' if has_cat else 'single series'}\")\n",
    "\n",
    "# overall\n",
    "overall_df = eval_block(merged)\n",
    "print(\"\\n=== Overall (TEST) ===\")\n",
    "print(overall_df.round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f53fe",
   "metadata": {},
   "source": [
    "### Hypothesis testing (Diebold–Mariano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cb767aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DM (MAE): stat=7.055, p=0.0000, n=4375\n",
      "DM (MSE): stat=5.180, p=0.0000, n=4375\n",
      "[MAE] p < 0.05 ⇒ reject H0 (equal accuracy). Model 2 (hybrid) better.\n",
      "[MSE] p < 0.05 ⇒ reject H0 (equal accuracy). Model 2 (hybrid) better.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Set function \n",
    "def _newey_west_var(d, lag):\n",
    "    \"\"\"HAC variance estimator with Bartlett weights.\"\"\"\n",
    "    d = np.asarray(d, float)\n",
    "    d = d[np.isfinite(d)]\n",
    "    n = d.size\n",
    "    if n < 2:\n",
    "        return np.nan\n",
    "    d = d - d.mean()\n",
    "    gamma0 = np.dot(d, d) / n\n",
    "    var = gamma0\n",
    "    for k in range(1, min(lag, n-1) + 1):\n",
    "        w = 1.0 - k/(lag+1.0)  # Bartlett weight\n",
    "        cov = np.dot(d[k:], d[:-k]) / n\n",
    "        var += 2.0 * w * cov\n",
    "    return var\n",
    "\n",
    "def diebold_mariano(y, yhat1, yhat2, loss=\"mse\", h=1, automatic_lag=True, alternative=\"two-sided\"):\n",
    "    \"\"\"\n",
    "    DM test comparing forecast 1 vs 2.\n",
    "      loss: 'mse' (squared error) or 'mae' (absolute error)\n",
    "      h   : forecast horizon; also used as HAC lag if automatic_lag=False\n",
    "      automatic_lag: if True, uses floor(1.1447*n^(1/3)) as NW lag; else uses h-1\n",
    "      alternative: 'two-sided' | 'greater' (model1 worse) | 'less' (model1 better)\n",
    "    Returns: (dm_stat, p_value, n_effective)\n",
    "    \"\"\"\n",
    "    y     = np.asarray(y,     float)\n",
    "    f1    = np.asarray(yhat1, float)\n",
    "    f2    = np.asarray(yhat2, float)\n",
    "\n",
    "    mask = np.isfinite(y) & np.isfinite(f1) & np.isfinite(f2)\n",
    "    y, f1, f2 = y[mask], f1[mask], f2[mask]\n",
    "    n = y.size\n",
    "    if n < 5:\n",
    "        return np.nan, np.nan, n\n",
    "\n",
    "    e1 = y - f1\n",
    "    e2 = y - f2\n",
    "    if loss.lower() == \"mse\":\n",
    "        L1, L2 = e1**2, e2**2\n",
    "    elif loss.lower() == \"mae\":\n",
    "        L1, L2 = np.abs(e1), np.abs(e2)\n",
    "    else:\n",
    "        raise ValueError(\"loss must be 'mse' or 'mae'\")\n",
    "\n",
    "    d = L1 - L2  # loss differential (positive => model1 worse)\n",
    "\n",
    "    # HAC variance\n",
    "    if automatic_lag:\n",
    "        # Andrews (1991) type rule-of-thumb used widely: floor(1.1447 * n^(1/3))\n",
    "        lag = int(np.floor(1.1447 * (n ** (1/3))))\n",
    "    else:\n",
    "        lag = max(0, h - 1)\n",
    "\n",
    "    s2 = _newey_west_var(d, lag)\n",
    "    if not np.isfinite(s2) or s2 <= 0:\n",
    "        return np.nan, np.nan, n\n",
    "\n",
    "    dm_stat = d.mean() / np.sqrt(s2 / n)\n",
    "\n",
    "    # p-value\n",
    "    if alternative == \"two-sided\":\n",
    "        pval = 2.0 * (1.0 - norm.cdf(abs(dm_stat)))\n",
    "    elif alternative == \"greater\":\n",
    "        pval = 1.0 - norm.cdf(dm_stat)\n",
    "    elif alternative == \"less\":\n",
    "        pval = norm.cdf(dm_stat)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'greater', or 'less'\")\n",
    "\n",
    "    return float(dm_stat), float(pval), int(n)\n",
    "\n",
    "\n",
    "dm_mae = diebold_mariano(\n",
    "    merged['actual_demand'],\n",
    "    merged['forecast_demand_mcr'],\n",
    "    merged['forecast_demand_hyb'],\n",
    "    loss=\"mae\", automatic_lag=True  \n",
    ")\n",
    "\n",
    "dm_mse = diebold_mariano(\n",
    "    merged['actual_demand'],\n",
    "    merged['forecast_demand_mcr'],\n",
    "    merged['forecast_demand_hyb'],\n",
    "    loss=\"mse\", automatic_lag=True\n",
    ")\n",
    "\n",
    "print(f\"DM (MAE): stat={dm_mae[0]:.3f}, p={dm_mae[1]:.4f}, n={dm_mae[2]}\")\n",
    "print(f\"DM (MSE): stat={dm_mse[0]:.3f}, p={dm_mse[1]:.4f}, n={dm_mse[2]}\")\n",
    "\n",
    "# Interpretation\n",
    "def interpret(dm_tuple, name):\n",
    "    stat, p, n = dm_tuple\n",
    "    if not np.isfinite(stat) or not np.isfinite(p):\n",
    "        print(f\"[{name}] DM is undefined (check data; n={n}).\")\n",
    "        return\n",
    "    if p < 0.05:\n",
    "        direction = \"Model 2 (hybrid) better\" if stat > 0 else \"Model 1 (MCR) better\"\n",
    "        print(f\"[{name}] p < 0.05 ⇒ reject H0 (equal accuracy). {direction}.\")\n",
    "    else:\n",
    "        print(f\"[{name}] p ≥ 0.05 ⇒ cannot reject equal predictive accuracy.\")\n",
    "\n",
    "interpret(dm_mae, \"MAE\")\n",
    "interpret(dm_mse, \"MSE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e4efc",
   "metadata": {},
   "source": [
    "### Demand-shock scenarios (-80%, -50%, -30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31e904b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Scenario Robustness (Degradation vs Base) ===\n",
      " Model    Scenario  RMSE_Degradation_%  MAE_Degradation_%  Coverage_Change_pp  Winkler_Change_%\n",
      "   MCR     drop_30               -34.2              -30.9                12.1             -38.9\n",
      "MCR+ML     drop_30                -3.6                4.9                 0.0               4.9\n",
      "   MCR     drop_50               -47.3              -43.4                19.7             -54.3\n",
      "MCR+ML     drop_50                22.3               35.9                 0.0              36.0\n",
      "   MCR lockdown_80                -4.1               -6.6                 4.2              -6.7\n",
      "MCR+ML lockdown_80                15.2               13.9                 0.0              13.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/xr3xsq8s2hl_27kvzqb6mkz40000gn/T/ipykernel_80062/94727038.py:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2 10.2\n",
      " 10.2 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6 32.6\n",
      " 32.6 32.6  5.2  5.2  5.2  5.2  5.2  5.2  5.2  5.2  5.2  5.2  5.2  5.2\n",
      "  5.2  5.2  5.2  7.4  7.4  7.4  7.4  7.4  7.4  7.4  7.4  7.4  7.4  7.4\n",
      "  7.4  7.4  7.4  7.4 14.  14.  14.  14.  14.  14.  14.  14.  14.  14.\n",
      " 14.  14.  14.  14.  14.   6.8  6.8  6.8  6.8  6.8  6.8  6.8  6.8  6.8\n",
      "  6.8  6.8  6.8  6.8  6.8  6.8  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   5.4  5.4  5.4  5.4  5.4\n",
      "  5.4  5.4  5.4  5.4  5.4  5.4  5.4  5.4  5.4  5.4  9.4  9.4  9.4  9.4\n",
      "  9.4  9.4  9.4  9.4  9.4  9.4  9.4  9.4  9.4  9.4  9.4  8.4  8.4  8.4\n",
      "  8.4  8.4  8.4  8.4  8.4  8.4  8.4  8.4  8.4  8.4  8.4  8.4  2.4  2.4\n",
      "  2.4  2.4  2.4  2.4  2.4  2.4  2.4  2.4  2.4  2.4  2.4  2.4 25.4 25.4\n",
      " 25.4 25.4 25.4 25.4 25.4 25.4 25.4 25.4 25.4 25.4 25.4 25.4  6.4  6.4\n",
      "  6.4  6.4  6.4  6.4  6.4  6.4  6.4  6.4  6.4  6.4  6.4  6.4  0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.2  1.2\n",
      "  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  4.2  4.2\n",
      "  4.2  4.2  4.2  4.2  4.2  4.2  4.2  4.2  4.2  4.2  4.2  4.2  3.   3.\n",
      "  3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   3.   8.6  8.6\n",
      "  8.6  8.6  8.6  8.6  8.6  8.6  8.6  8.6  8.6  8.6  8.6  8.6 21.4 21.4\n",
      " 21.4 21.4 21.4 21.4 21.4 21.4 21.4 21.4 21.4 21.4 21.4 21.4  7.   7.\n",
      "  7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7.   7. ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  g.loc[mask, \"actual_demand\"] = g.loc[mask, \"actual_demand\"] * (1.0 - drop_pct)\n"
     ]
    }
   ],
   "source": [
    "# Metrics \n",
    "def metrics(y, yhat, low, up, alpha=0.20):\n",
    "    y = np.asarray(y, float); yhat = np.asarray(yhat, float)\n",
    "    low = np.asarray(low, float); up = np.asarray(up, float)\n",
    "    err = yhat - y\n",
    "    rmse = float(np.sqrt(np.mean(err**2)))\n",
    "    mae  = float(np.mean(np.abs(err)))\n",
    "    coverage = float(np.mean((y >= low) & (y <= up)))\n",
    "    width = float(np.mean(up - low))\n",
    "    penalty = (2/alpha)*((low-y)*(y<low) + (y-up)*(y>up))  # Winkler (lower is better)\n",
    "    winkler = float(np.mean((up - low) + penalty))\n",
    "    return rmse, mae, coverage, width, winkler\n",
    "\n",
    "# Evaluate a dataframe for both models\n",
    "def eval_two_models(df, alpha=0.20):\n",
    "    y = df[\"actual_demand\"]\n",
    "    out = {}\n",
    "\n",
    "    # MCR\n",
    "    rmse, mae, cov, wid, wkl = metrics(\n",
    "        y, df[\"forecast_demand_mcr\"], df[\"p10_mcr\"], df[\"p90_mcr\"], alpha\n",
    "    )\n",
    "    out[\"MCR\"] = dict(RMSE=rmse, MAE=mae, Coverage=cov, Width=wid, Winkler=wkl)\n",
    "\n",
    "    # Hybrid\n",
    "    rmse, mae, cov, wid, wkl = metrics(\n",
    "        y, df[\"forecast_demand_hyb\"], df[\"p10_hyb\"], df[\"p90_hyb\"], alpha\n",
    "    )\n",
    "    out[\"MCR+ML\"] = dict(RMSE=rmse, MAE=mae, Coverage=cov, Width=wid, Winkler=wkl)\n",
    "    return out\n",
    "\n",
    "\n",
    "baseline = eval_two_models(merged, alpha=ALPHA)\n",
    "\n",
    "# scenario constructors \n",
    "def apply_uniform_drop(df, drop_pct):\n",
    "    \"\"\"\n",
    "    drop_pct = 0.30 -> 30% drop (multiply actuals by 0.70)\n",
    "    \"\"\"\n",
    "    g = df.copy()\n",
    "    g[\"actual_demand\"] = g[\"actual_demand\"] * (1.0 - drop_pct)\n",
    "    return g\n",
    "\n",
    "def apply_lockdown(df, drop_pct=0.80, weeks=8, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Reduce actuals by drop_pct in a contiguous block (default center 8 weeks).\n",
    "    If start_date/end_date provided, they define the lockdown window.\n",
    "    \"\"\"\n",
    "    g = df.copy()\n",
    "    idx = g[\"date\"].sort_values().unique()\n",
    "    if start_date is None or end_date is None:\n",
    "        # center window\n",
    "        n = len(idx)\n",
    "        w = min(weeks, n)\n",
    "        s = max(0, (n - w)//2)\n",
    "        mask_dates = set(idx[s:s+w])\n",
    "    else:\n",
    "        mask_dates = set(pd.date_range(pd.to_datetime(start_date),\n",
    "                                       pd.to_datetime(end_date), freq=\"W-SUN\"))\n",
    "    mask = g[\"date\"].isin(mask_dates)\n",
    "    g.loc[mask, \"actual_demand\"] = g.loc[mask, \"actual_demand\"] * (1.0 - drop_pct)\n",
    "    return g\n",
    "\n",
    "# Evaluate scenarios \n",
    "scenarios = {\n",
    "    \"drop_30\": apply_uniform_drop(merged, 0.30),\n",
    "    \"drop_50\": apply_uniform_drop(merged, 0.50),\n",
    "    \"lockdown_80\": apply_lockdown(merged, drop_pct=0.80, weeks=8)  # adjust weeks/dates as needed\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for scen_name, scen_df in scenarios.items():\n",
    "    res = eval_two_models(scen_df, alpha=ALPHA)\n",
    "    for model in [\"MCR\", \"MCR+ML\"]:\n",
    "        base = baseline[model]\n",
    "        cur  = res[model]\n",
    "        rows.append({\n",
    "            \"Model\": model,\n",
    "            \"Scenario\": scen_name,\n",
    "            # % degradation vs baseline: positive = worse\n",
    "            \"RMSE_Degradation_%\": 100.0 * (cur[\"RMSE\"] / base[\"RMSE\"] - 1.0),\n",
    "            \"MAE_Degradation_%\":  100.0 * (cur[\"MAE\"]  / base[\"MAE\"]  - 1.0),\n",
    "            # coverage change in percentage points\n",
    "            \"Coverage_Change_pp\": 100.0 * (cur[\"Coverage\"] - base[\"Coverage\"]),\n",
    "            # Winkler: larger is worse — report % change\n",
    "            \"Winkler_Change_%\":   100.0 * (cur[\"Winkler\"] / base[\"Winkler\"] - 1.0),\n",
    "        })\n",
    "\n",
    "robustness_tbl = pd.DataFrame(rows)\n",
    "print(\"\\n=== Scenario Robustness (Degradation vs Base) ===\")\n",
    "print(robustness_tbl.to_string(index=False, float_format=lambda x: f\"{x:,.1f}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bae99",
   "metadata": {},
   "source": [
    "### Uncertainty Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71774851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Uncertainty Sensitivity (Spread) ===\n",
      " Model  Avg_Width  Rel_Width_vs_Forecast\n",
      "   MCR     37.965                  2.404\n",
      "MCR+ML      0.053                    NaN\n"
     ]
    }
   ],
   "source": [
    "# Uncertainty Sensitivity\n",
    "rows = []\n",
    "for model in [\"MCR\", \"MCR+ML\"]:\n",
    "    if model == \"MCR\":\n",
    "        low, up, mean = merged[\"p10_mcr\"], merged[\"p90_mcr\"], merged[\"forecast_demand_mcr\"]\n",
    "    else:\n",
    "        low, up, mean = merged[\"p10_hyb\"], merged[\"p90_hyb\"], merged[\"forecast_demand_hyb\"]\n",
    "    \n",
    "    width = (up - low).to_numpy()\n",
    "    avg_width = np.mean(width)\n",
    "    rel_width = np.mean(width / np.where(mean != 0, mean, np.nan))\n",
    "    \n",
    "    rows.append({\n",
    "        \"Model\": model,\n",
    "        \"Avg_Width\": avg_width,\n",
    "        \"Rel_Width_vs_Forecast\": rel_width\n",
    "    })\n",
    "\n",
    "uncertainty_tbl = pd.DataFrame(rows)\n",
    "print(\"\\n=== Uncertainty Sensitivity (Spread) ===\")\n",
    "print(uncertainty_tbl.round(3).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fe0cf",
   "metadata": {},
   "source": [
    "### Error decomposition (bias/variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3767953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bias–Variance ===\n",
      " Model    Bias  Variance\n",
      "   MCR -14.833   935.315\n",
      "MCR+ML  -1.743   415.596\n"
     ]
    }
   ],
   "source": [
    "def bias_variance_from_df(df, y_col, yhat_col):\n",
    "    \"\"\"Return Bias, Var(errors), and MSE for a model given actuals and forecasts.\"\"\"\n",
    "    y = np.asarray(df[y_col], float)\n",
    "    yhat = np.asarray(df[yhat_col], float)\n",
    "    mask = np.isfinite(y) & np.isfinite(yhat)\n",
    "    e = yhat[mask] - y[mask]\n",
    "    if e.size == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    bias = float(e.mean())\n",
    "    var_err = float(e.var(ddof=0))     # population variance over horizon\n",
    "    mse = float((e**2).mean())\n",
    "    # sanity: mse ≈ bias^2 + var_err (up to rounding)\n",
    "    return bias, var_err, mse\n",
    "\n",
    "# Overall bias–variance table\n",
    "rows = []\n",
    "b_mcr, v_mcr, mse_mcr = bias_variance_from_df(\n",
    "    merged, \"actual_demand\", \"forecast_demand_mcr\"\n",
    ")\n",
    "rows.append({\"Model\":\"MCR\", \"Bias\": b_mcr, \"Variance\": v_mcr, \"MSE\": mse_mcr})\n",
    "\n",
    "b_hyb, v_hyb, mse_hyb = bias_variance_from_df(\n",
    "    merged, \"actual_demand\", \"forecast_demand_hyb\"\n",
    ")\n",
    "rows.append({\"Model\":\"MCR+ML\", \"Bias\": b_hyb, \"Variance\": v_hyb, \"MSE\": mse_hyb})\n",
    "\n",
    "bv_table = pd.DataFrame(rows)\n",
    "print(\"\\n=== Bias–Variance ===\")\n",
    "print(bv_table[[\"Model\",\"Bias\",\"Variance\"]].round(3).to_string(index=False))\n",
    "\n",
    "# Breakdown per-category\n",
    "if \"category\" in merged.columns:\n",
    "    per_cat = []\n",
    "    for cat, g in merged.groupby(\"category\"):\n",
    "        b, v, mse = bias_variance_from_df(g, \"actual_demand\", \"forecast_demand_mcr\")\n",
    "        per_cat.append({\"Category\":cat, \"Model\":\"MCR\", \"Bias\":b, \"Variance\":v, \"MSE\":mse})\n",
    "        b, v, mse = bias_variance_from_df(g, \"actual_demand\", \"forecast_demand_hyb\")\n",
    "        per_cat.append({\"Category\":cat, \"Model\":\"MCR+ML\", \"Bias\":b, \"Variance\":v, \"MSE\":mse})\n",
    "    per_cat_table = pd.DataFrame(per_cat)\n",
    "    # Sort categories by absolute bias or variance \n",
    "    per_cat_table = per_cat_table.sort_values([\"Model\",\"Variance\"], ascending=[True,False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d672d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
